# 矩阵分析上篇

## 一：线性代数基础

### 1.1 tensor

* tensor是一个多维的矩阵

![6_1.png](https://i.loli.net/2019/06/03/5cf4d2838898890313.png)

* 将tensor转换为一个矩阵，（分别按照三个坐标轴展开，即图中红，绿，蓝三条轴）

![6_2.png](https://i.loli.net/2019/06/03/5cf4d2847a45288684.png)

### 1.2 $\mathbf{Ax=b}$的行视图

* 假设$A \in \mathbb{R}^{m \times n}$
  $$
  \underbrace{\left[\begin{array}{cc}{2} & {-1} \\ {1} & {1}\end{array}\right]}_{A \in \mathbb{R}^{2 \times 2}} \underbrace{\left[\begin{array}{c}{x} \\ {y}\end{array}\right]}_{x \in \mathbb{R}^{2}}=\underbrace{\left[\begin{array}{l}{1} \\ {5}\end{array}\right]}_{b \in \mathbb{R}^{2}}
  $$

$$
\underbrace{\left[\begin{array}{ccc}{2} & {1} & {1} \\ {4} & {-6} & {0} \\ {-2} & {7} & {2}\end{array}\right]}_{A \in \mathbb{R}^{3 \times 3}} \underbrace{\left[\begin{array}{c}{u} \\ {v} \\ {w}\end{array}\right]}_{x \in \mathbb{R}^{3}}=\underbrace{\left[\begin{array}{c}{5} \\ {-2} \\ {9}\end{array}\right]}_{b \in \mathbb{R}^{3}}
$$

* 行视图：可以理解为凸优化中的超平面，每一行代表一个超平面（二维中是直线，三维中是平面，高维中是超平面）

  ![6_3.png](https://i.loli.net/2019/06/03/5cf4d494e587d32974.png)

  

### 1.3 $\mathbf{Ax=b}$的列视图

* 列视图可以理解为矩阵列的线性组合。解方程相当于解线性组合前的系数。
  $$
  x\left[\begin{array}{l}{2} \\ {1}\end{array}\right]+y\left[\begin{array}{c}{-1} \\ {1}\end{array}\right]=\left[\begin{array}{l}{1} \\ {5}\end{array}\right]
  $$

![6_4.png](https://i.loli.net/2019/06/03/5cf4d5c59cfab22257.png)

### 1.4 线性相关和线性无关

* 线性相关：矢量集合$\left[\mathbf{a}_{1}, \dots, \mathbf{a}_{n}\right]$是线性相关的，如果$\sum_{k=1}^{n} c_{k} a_{k}=0$，当且仅当$c_{1}, c_{2}, \ldots, c_{n}\neq 0$,即至少有一个向量可以由其它向量线性导出(如下式)，换句话说，就是不能是所有的系数同时为0.
  $$
  \mathbf{a}_{l}=-\frac{1}{c_{l}} \sum_{k=1, k \neq 1}^{n} c_{k} a_{k}
  $$

* 线性无关：矢量集合$\left[\mathbf{a}_{1}, \dots, \mathbf{a}_{n}\right]$是线性无关的，如果$\sum_{k=1}^{n} c_{k} a_{k}=0$，当且仅当$c_{1}, c_{2}, \ldots, c_{n}= 0$
* 定义$\mathbf{A}=\left[\mathbf{a}_{1}, \dots, \mathbf{a}_{n}\right]$，则$\mathbf{Ax}=0$只有$\mathbf{x=0}$，没有其它的线性组合能产生$\mathbf{0}$，此时$\mathbf{A}$可逆。此时A中的向量线性无关。

### 1.5 Span、基和子空间

* span（子空间）:
  $$
  \operatorname{span}\left[\mathbf{a}_{1}, \ldots, \mathbf{a}_{n}\right]=\left\{\mathbf{y} \in \mathbb{R}^{m} | \mathbf{y}=\sum_{k=1}^{n} c_{k} a_{k}\right\}=S
  $$

* 其实span是向量集合$\left[\mathbf{a}_{1}, \dots, \mathbf{a}_{n}\right]$所有的线性组合。此时如果$\left[\mathbf{a}_{1}, \dots, \mathbf{a}_{n}\right]$是线性无关的，那么$\left[\mathbf{a}_{1}, \dots, \mathbf{a}_{n}\right]$是S的一组基。
* 正交基是指$\mathbf{a_{i}^{T}a_{j}}=0$，即基中的向量不仅线性无关，而且两两正交。
* S可以有不同的一组基，但是基里向量的个数是相同的，被称为S的维数。等于rank(A)。
* 一个子空间用一组基就可以表示了！
* 对于基的理解：恰到好处。基中向量的个数是子空间的维数，也是最大线性无关向量的个数。子空间中的任何一个向量都可以用一组基的线性组合来表示。

## 二：线性代数精华

四个基本的子空间，包括列空间，零空间，行空间，左零空间。

### 2.1 列空间

$\mathbf{A}$是一个$m*n$的矩阵，$\mathbf{C(A)}$是$\mathbf{R^{m}}(not\ \mathbf{R^{n}})$的子空间。$\mathbf{C(A)}$包含所有列的线性组合，即$\mathbf{C(A)}=\{\mathbf{y=Ax,x \in R^{n}}\}$.
$$
A=\left[\begin{array}{ll}{1} & {0} \\ {4} & {3} \\ {2} & {3}\end{array}\right], C(A)=\operatorname{span}\left[\left[\begin{array}{l}{1} \\ {4} \\ {2}\end{array}\right],\left[\begin{array}{l}{0} \\ {3} \\ {3}\end{array}\right]\right],构成了一个R^{3}的子空间。
$$
其实，该列空间$\mathbf{C(A)}$是全空间的一个平面而已，是全空间中的一部分。列的线性组合无法组合出平面外的任何向量。

![6_5.png](https://i.loli.net/2019/06/05/5cf717aac1d7e93181.png)

### 2.2 零空间

$\mathbf{A}$是一个$m*n$的矩阵，$\mathbf{N(A)}$是$\mathbf{R^{n}}(not\ \mathbf{R^{m}})$的子空间。

定义：$\mathbf{N(A)}$是包含$\mathbf{Ax=0}$的所有解的集合。注意：$\mathbf{Ax=b}$的解并不形成一个子空间（因为不包含0向量）。

例子：
$$
A=\left[\begin{array}{llll}{1} & {2} & {2} & {4} \\ {3} & {8} & {6} & {16}\end{array}\right] \rightarrow U=\left[\begin{array}{llll}{1} & {2} & {2} & {4} \\ {0} & {2} & {0} & {4}\end{array}\right]
$$
即$\mathbf{Ux=0}$，于是有$S_{1}=\left[\begin{array}{c}{-2} \\ {0} \\ {1} \\ {0}\end{array}\right] S_{2}=\left[\begin{array}{c}{0} \\ {-2} \\ {0} \\ {1}\end{array}\right]$

因此$N(A)=C\left(\left[\begin{array}{c}{-2} \\ {0} \\ {1} \\ {0}\end{array}\right],\left[\begin{array}{c}{0} \\ {-2} \\ {0} \\ {1}\end{array}\right]\right)$，是$\mathbf{R^{4}}$的子空间。

### 2.3 行空间

行空间（row space）:$\mathbf{C(A^{T})}$是$\mathbf{R^{n}}$的子空间

定义：包含所有行的线性组合。
$$
A=\left[\begin{array}{llll}{1} & {2} & {2} & {4} \\ {3} & {8} & {6} & {16}\end{array}\right], 则C\left(A^{\top}\right)=C\left(\left[\begin{array}{l}{1} \\ {2} \\ {2} \\ {4}\end{array}\right],\left[\begin{array}{c}{3} \\ {8} \\ {6} \\ {16}\end{array}\right]\right)
$$

### 2.4 左零空间

$\mathrm{N}\left(\mathrm{A}^{\top}\right)=\left\{\mathrm{A}^{\top} \mathrm{y}=0\right\}$的解的集合。是$\mathbf{R^{m}}$的子空间。

### 2.5 四个基本的子空间的关系

![6_6.jpg](https://i.loli.net/2019/06/05/5cf723c35842c11693.jpg)

### 2.6 利用子空间重新看待方程组的解

![6_7.png](https://i.loli.net/2019/06/05/5cf7257f7f7c850787.png)

* 如果有解，解的形式为：$\mathbf{x=p+v}$，其中$\mathbf{p}$为特解（$\mathbf{Ap=b}$)，$\mathbf{v}$是通解($\mathbf{Av=0}$)。

## 三：特征分解

### 3.1 方阵的特征值与特征向量

给定一个矩阵$A=\left[\begin{array}{ll}{4} & {1} \\ {1} & {4}\end{array}\right]$，对于$\mathbf{X}_{1}=\left[\begin{array}{l}{1} \\ {0}\end{array}\right]$，则有$\mathbf{A x_{1}}=\left[\begin{array}{l}{4} \\ {1}\end{array}\right]$，对于$\mathbf{X}_{3}=\left[\begin{array}{l}{1} \\ {1}\end{array}\right]$，则有$\mathbf{A x_{3}}=5\left[\begin{array}{l}{1} \\ {1}\end{array}\right]$。

特征值可以理解为对一个向量的伸缩程度。

![6_8.png](https://i.loli.net/2019/06/05/5cf7610e645d617232.png)

### 3.2 特征分解的一般性质

* $\mathbf{Ax=\lambda x}$，其中$\lambda$为特征值，$\mathbf{x}$为特征向量。

* 求取特征值的过程：$\mathbf{(A-\lambda I)x=0}$,$det(\mathbf{A-\lambda I})=0$.即保证其不可逆（行列式的值为0）

* 对于$\mathbf{Ax_{i}=\lambda x_{i}}$,如果所有的特征值都不相同，则相应的所有的特征向量线性无关。此时A可以被对角化为：
  $$
  \mathbf{A=V\Lambda V^{-1}}
  $$
  其中：$\mathrm{V}=\left[\mathrm{x}_{1}, \ldots, \mathrm{x}_{\mathrm{n}}\right], \quad \Lambda=\operatorname{Diag}\left(\lambda_{1}, \ldots, \lambda_{\mathrm{n}}\right)$,$\mathbf{x_i}$是特征向量，$\lambda$为特征值。

* 注意：并不是所有的方阵都可以被对角化。当特征值中有重根时，便不一定可以被对角化。

### 3.3 对称矩阵的特征分解

* 一个对称矩阵，无论其特征值相同或者不同，则其相应的所有的特征向量正交（$U U^{\top}=U^{\top} U=I$）

* 正交意味着：任意两个特征向量垂直，每个特征向量的模长为1。

  此时进行特征分解：

  ![6_9.png](https://i.loli.net/2019/06/05/5cf769066d88a97259.png)

* 对称矩阵的特征值是实数
  
* 如果$\mathbf{A \in R^{n \times n}}$,是一个对称矩阵且 $rank r \leq n$,则有：
  $$
  \underbrace{\left|\lambda_{1}\right| \geq\left|\lambda_{2}\right| \geq \ldots \geq\left|\lambda_{\mathrm{r}}\right|}_{r}>\underbrace{\lambda_{\mathrm{r}+1}=\ldots \lambda_{\mathrm{n}}}_{n-\mathrm{r}}=0 \\
  \operatorname{Rank}\left(\mathbf{A}^{\top} \mathbf{A}\right)=\operatorname{Rank}\left(\mathbf{A} \mathbf{A}^{\top}\right)=\operatorname{Rank}(\mathbf{A})=\operatorname{Rank}(\Lambda)
  $$
  
* 特征值在某些程度上可以反应能量的大小，在某些时候，后面较小的特征值可以删除。

### 3.4 特征分解和子空间的关系

![6_10.jpg](https://i.loli.net/2019/06/05/5cf770dfdede870970.jpg)

## 四：PCA

### 4.1 优化问题

![6_11.png](https://i.loli.net/2019/06/05/5cf77667aa60187187.png)

则：$\mathbf{x^{T}Ax = \lambda x^{T}x =  \lambda ||x||_2^{2} =  \lambda}$,因此，原问题变为求A的最大特征值。

### 4.2 正交变换

![6_12.png](https://i.loli.net/2019/06/05/5cf777362c6d358850.png)

* 降维：维度降低，但是样本数量不变

### 4.3 PCA

![6_13.png](https://i.loli.net/2019/06/05/5cf777ad0a8b020166.png)

* 选取原则可以这样理解：前$d^{'}$个特征值的能量占到所有特征值能量的95%。
* 转换矩阵的求解方法：
  1. 求原矩阵的协方差矩阵
  2. 对协方差矩阵进行特征分解，
  3. 对特征值进行排序，
  4. 找出较大的特征值对应的特征向量，即可组成转换矩阵
* 用方差来衡量点的分散程度。希望投影后的值越分散越好。如果所有数据都投影到某个集中的未知上，则数据信息丢失过多。希望方差尽可能大是为了尽可能地是数据区分开，从而尽可能多地保留数据地原始信息。所以数据越分散越好。

### 4.4 PCA举例

![6_14.png](https://i.loli.net/2019/06/05/5cf779a1bc7bd89780.png)

* 注意有5个样本，每个样本是2维。