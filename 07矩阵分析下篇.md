# 矩阵分析下篇

## 一：SVD理论(奇异值分解)

SVD是特征分解的广义化。特征分解本质上是对方阵而言，而SVD对任意矩阵而言。

任何$\mathbf{A \in R^{m \times n}}$的矩阵都可以被分解为：
$$
\mathbf{A = U \Sigma V^{T}}
$$
有几点需要注意：

1. $\mathbf{U \in R^{m \times m}}$和$\mathbf{V \in R^{n \times n}}$是正交矩阵。这两个矩阵都是方阵。
2. $\Sigma$是一个$m \times n$的矩阵，和矩阵$\mathbf{A}$的样子是一样的。

定义 $p = min(m,n)$,则有
$$
\Sigma(\mathrm{i}, \mathrm{j})=\left\{\begin{array}{ll}{\sigma_{\mathrm{i}},} & {\mathrm{i}=\mathrm{j}} \\ {0,} & {\mathrm{i} \neq \mathrm{j}}\end{array}\right.\\
\sigma_{1} \geq \sigma_{2} \geq \ldots \geq \sigma_{\mathrm{p}} \geq 0
$$
因此：
$$
\Sigma=\left[\begin{array}{cc}{\Sigma_{p}} & {0} \\ {0} & {0}\end{array}\right]
$$

## 二：SVD分解的三种形式

### 2.1 第一种形式：分块型

假定r表示非零奇异值的数量，即$r=rank(\mathbf{A})$，对奇异值进行排序：
$$
\underbrace{\sigma_{1} \geq \ldots \geq \sigma_{\mathrm{r}}}_{r}>\underbrace{\sigma_{\mathrm{r}+1}=\ldots=\sigma_{\mathrm{p}}}_{\mathrm{p}-\mathrm{r}}=0
$$
说明：

* 矩阵本来有p个奇异值（p是m和n中的最小值），但是有r个非零奇异值。

分块式SVD可以表示为：
$$
\mathbf{A=U \Sigma V^{\top}} \\
= \underbrace{\left[\mathrm{U}_{1} \mathrm{U}_{2}\right]}_{\mathbf{U}}\underbrace{\left[\begin{array}{cc}{\Sigma_{1}} & {0_{r \times(n-r)}} \\ {0_{(m-r) \times r}} & {0_{(m-r) \times(n-r)}}\end{array}\right]}_{\Sigma}
\underbrace{\left[\begin{array}{c}{\mathbf{V}_{1}^{\top}} \\ {\mathbf{V}_{2}^{\top}}\end{array}\right]}_{\mathbf{V^{T}}}
$$
其中：$\Sigma_{1}=\operatorname{diag}\left(\sigma_{1}, \ldots, \sigma_{\mathrm{r}}\right)$且有$\sigma_r>0$,$\Sigma_{1}$是由正奇异值组成的对角矩阵。$\mathrm{U}_{1} \in \mathbb{R}^{\mathrm{mxr}}$对应于r个非零奇异值的左奇异矩阵，$\mathrm{U}_{2} \in \mathbb{R}^{\mathrm{m} \times(\mathrm{m}-\mathrm{r})}$对应于0奇异值的左奇异矩阵。

### 2.2 第二种形式：迷你型

![7_1.png](https://i.loli.net/2019/06/10/5cfdafd5d489089260.png)

### 2.3 第三种形式：外积型

$$
\begin{aligned} \mathbf{A} &=\mathbf{U \Sigma V^{T}} \\ &=\sum_{i=1}^{r} \sigma_{i} \mathbf{u_{i}} \mathbf{v_{i}^{T}} \end{aligned}
$$

## 三：SVD和特征分解的关系

* 已知$\mathbf{A=U \Sigma V^{T}}$,则：
  $$
  \mathbf{AA^{T}=U \Sigma V^{T}V\Sigma^{T}U^{T}=U \Sigma \Sigma^{T}U^{T}=U \Lambda_{L}U^{T}}
  $$
  其中
  $$
  \Lambda_{L}=\left[\begin{array}{cc}{\Sigma_{1}^{2}} & {0} \\ {0} & {0}\end{array}\right]
  $$

所以可以得出：U是$\mathbf{AA^{T}}$的特征矩阵，$\sigma_{1}^{2}, \ldots, \sigma_{\mathrm{r}}^{2}, 0 \ldots 0$是特征值。

因此：
$$
\sigma_{\mathrm{k}}=\sqrt{\lambda_{\mathrm{k}}\left(\mathrm{AA}^{\top}\right)}=\sqrt{\lambda_{\mathrm{k}}\left(\mathrm{A}^{\top} \mathrm{A}\right)}
$$
所以：A矩阵的奇异值是$\mathbf{AA^{T}}$矩阵特征值开根号。

同理：$\mathbf{V}$是$\mathbf{A^{T}A}$的特征矩阵。$\sigma_{1}^{2}, \ldots, \sigma_{\mathrm{r}}^{2}, 0 \ldots 0$是特征值。

说明：

* $\mathbf{AA^{T}}$是$m \times m$矩阵，有r个正特征值，为$\sigma_{1}^{2}, \ldots, \sigma_{\mathrm{r}}^{2},$这些特征值开根号，即为矩阵A的r个奇异值。然后有m-r个0奇异值。
* $\mathbf{A^{T}A}$是$n \times n$矩阵，有r个正特征值，为$\sigma_{1}^{2}, \ldots, \sigma_{\mathrm{r}}^{2},$这些特征值开根号，即为矩阵A的r个奇异值。然后有n-r个0奇异值。
* 所以$\mathbf{AA^{T}}$和$\mathbf{A^{T}A}$正特征值是相同的，区别在于0特征值的个数。

## 四：SVD和子空间的关系

### 4.1 和列空间的关系

![7_2.png](https://i.loli.net/2019/06/10/5cfdb9fc5a10c49154.png)

说明：

* $C(A)$是列空间，是A中列向量的线性组合。
* 因为x是任意向量，所以把$\mathbf{V_{1}^{T}x}$定义为$\mathbf{c_1}$,$\mathbf{V_{2}^{T}x}$定义为$\mathbf{c_2}$,$\mathbf{c_1}$和$\mathbf{c_2}$都是任意的列向量。
* 最后一步中：$\Sigma_{1}$是$r \times r$的对角矩阵,所以$\mathbf{\Sigma_{1}c_{1}}$是任意的列向量。

### 4.2 和零空间的关系

![7_3.png](https://i.loli.net/2019/06/10/5cfdbdf93d00331156.png)

说明：

* $\mathbf{c}$为一个任意的列向量。
* 因为$\mathbf{V}$是正交矩阵，所以$\mathbf{V_{1}^{T}}$中的行向量和$\mathbf{V_2}$中的列向量都是正交的，相乘得0.所以$\mathbf{V_{2}^{T}V_2=I}$
* $\mathbf{x=V_2c}$说明x可以由$V_2$中得列进行线性组合而来。
* $\mathbf{Ax=b}$的解为特解（p）加通解（z)。其中通解为$\mathbf{V_2c}$

### 4.3 再看四个子空间

![7_4.png](https://i.loli.net/2019/06/10/5cfdc2c6452c947778.png)

## 五：其它知识

### 5.1 投影

![7_5.png](https://i.loli.net/2019/06/10/5cfdc6d2213fc87445.png)

$min||\mathbf{Ax-b}||_{2}^{2}$，本质上是找到一个x，使得在A的列空间上的向量（Ax）与b向量之间的模长最小。由投影的观点可以发现，当模长最小时，这个**最优的$x^{*}$恰好使得b向量在列空间上的投影与$Ax^{*}$相等**。所以可知
$$
A^{T}(b-A \hat{x})=0\\
A^{T}b=A^{T}A \hat{x}\\
\hat{x}=(A^{T}A)^{-1}A^{T}b
$$
在优化部分，求解该最优值，我们采用梯度等于0的方法，也是求得上述结果。因此可以与线性回归求解系数进行对比理解。

### 5.2 行列式的几何意义

![7_6.png](https://i.loli.net/2019/06/10/5cfdcb12845d921293.png)

行列式为行列式中两个列向量围成的平行四边形的面积。对应到三维空间，就是一个平行六面体的面积（平行六面体可以理解为一个长方体歪了一下）

## 六： SVD实际应用

![7_7.png](https://i.loli.net/2019/06/10/5cfdcc7b2229619152.png)

因为A的秩为r，所以A有r个正奇异值。对这r个奇异值按照从大到小的顺序进行排列，取出前面比较大的k个奇异值，按照上图中的公式14进行计算，既可以得到降维后的矩阵。

所以步骤如下：

1. 对原矩阵进行奇异值分解
2. 进行奇异值排序
3. 选择较大的奇异值计算变换之后的矩阵。

**SVD和PCA的比较：**

* 在PCA中，我们首先对原矩阵求协方差矩阵，然后对协方差矩阵进行特征分解，求得特征值和对应的特征向量，然后选择较大的特征值对应的特征向量组成变换矩阵，进行变换。
* 在SVD中，我们直接对原矩阵进行奇异值分解，然后选择较大的奇异值及其对应的奇异向量，直接计算转换之后的矩阵。
* 在实际中，一定会用奇异值分解，因为从数值计算的角度来看，奇异值分解更加稳定。